{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benle1/Unlearn-Saliency/PEM_composition_img_gen\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "user = os.getenv('USER')\n",
    "%cd /home/{user}/Unlearn-Saliency/PEM_composition_img_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benle1/.local/lib/python3.11/site-packages/phasepack/tools.py:11: UserWarning: \n",
      "Module 'pyfftw' (FFTW Python bindings) could not be imported. To install it, try\n",
      "running 'pip install pyfftw' from the terminal. Falling back on the slower\n",
      "'fftpack' module for 2D Fourier transforms.\n",
      "  warnings.warn(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Union\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import cast_training_params, compute_snr\n",
    "from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "from diffusers import AutoPipelineForText2Image, StableDiffusionPipeline\n",
    "from huggingface_hub.repocard_data import EvalResult, ModelCardData\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from diffusers import AutoPipelineForText2Image, StableDiffusionPipeline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "from typing import Literal, List, Dict, Tuple, Optional, Callable, Union\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('..')  # TODO: configure this at apptainer level\n",
    "\n",
    "if is_wandb_available():\n",
    "    from libs.integrations.wandb import wandb_log_image\n",
    "from libs.integrations.tensorboard import tensorboard_log_image\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from libs.metrics import MetricImageTextSimilarity\n",
    "from libs.evaluator import EvaluatorTextToImage\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from pydantic import BaseModel\n",
    "from libs.utils.logger import get_logger, setup_loggers\n",
    "\n",
    "\n",
    "logger = get_logger('trainer')\n",
    "setup_loggers(modules_info=['libs.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-21 14:59:41 +0100] libs.trainer INFO     Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6055a3b082af4345aaec42a32e5254e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350f28809df84b029337ea053fac31da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/7632 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "value should be one of int, float, str, bool, or torch.Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 956\u001b[0m\n\u001b[1;32m    953\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mwait_for_everyone()\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[43mlaunch_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstable-diffusion-v1-5/stable-diffusion-v1-5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_forget_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./assets/imagenette_splits/n02979186/train_forget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_retain_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./assets/imagenette_splits/n02979186/train_retain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_warmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./assets/lora/imagenette_splits/n02979186\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpointing_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPicture of a church\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_validation_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m    976\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 950\u001b[0m, in \u001b[0;36mlaunch_training\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mis_local_main_process:\n\u001b[1;32m    949\u001b[0m         trainer \u001b[38;5;241m=\u001b[39m TrainerLora(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 950\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# Wait for all processes to finish\u001b[39;00m\n\u001b[1;32m    953\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mwait_for_everyone()\n",
      "Cell \u001b[0;32mIn[10], line 545\u001b[0m, in \u001b[0;36mTrainerLora.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# We need to initialize the trackers we use, and also store our configuration.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# The trackers initializes automatically on the main process.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39mis_main_process:\n\u001b[0;32m--> 545\u001b[0m     \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_trackers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2image-fine-tune\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Train!\u001b[39;00m\n\u001b[1;32m    548\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:771\u001b[0m, in \u001b[0;36mAccelerator.on_main_process.<locals>._inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_inner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPartialState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_main_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:2788\u001b[0m, in \u001b[0;36mAccelerator.init_trackers\u001b[0;34m(self, project_name, config, init_kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2787\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tracker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers:\n\u001b[0;32m-> 2788\u001b[0m         \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_init_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/tracking.py:79\u001b[0m, in \u001b[0;36mon_main_process.<locals>.execute_on_main_process\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_on_main_process\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_process_only\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPartialState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_main_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/tracking.py:211\u001b[0m, in \u001b[0;36mTensorBoardTracker.store_init_configuration\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;129m@on_main_process\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstore_init_configuration\u001b[39m(\u001b[38;5;28mself\u001b[39m, values: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Logs `values` as hyperparameters for the run. Should be run at the beginning of your experiment. Stores the\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    hyperparameters in a yaml file for future use.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m            `str`, `float`, `int`, or `None`.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_hparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    213\u001b[0m     project_run_name \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/tensorboard/writer.py:330\u001b[0m, in \u001b[0;36mSummaryWriter.add_hparams\u001b[0;34m(self, hparam_dict, metric_dict, hparam_domain_discrete, run_name, global_step)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(hparam_dict) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(metric_dict) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparam_dict and metric_dict should be dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 330\u001b[0m exp, ssi, sei \u001b[38;5;241m=\u001b[39m \u001b[43mhparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparam_domain_discrete\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_name:\n\u001b[1;32m    333\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/tensorboard/summary.py:318\u001b[0m, in \u001b[0;36mhparams\u001b[0;34m(hparam_dict, metric_dict, hparam_domain_discrete)\u001b[0m\n\u001b[1;32m    316\u001b[0m         hps\u001b[38;5;241m.\u001b[39mappend(HParamInfo(name\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mDataType\u001b[38;5;241m.\u001b[39mValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_TYPE_FLOAT64\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue should be one of int, float, str, bool, or torch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    322\u001b[0m content \u001b[38;5;241m=\u001b[39m HParamsPluginData(session_start_info\u001b[38;5;241m=\u001b[39mssi, version\u001b[38;5;241m=\u001b[39mPLUGIN_DATA_VERSION)\n\u001b[1;32m    323\u001b[0m smd \u001b[38;5;241m=\u001b[39m SummaryMetadata(\n\u001b[1;32m    324\u001b[0m     plugin_data\u001b[38;5;241m=\u001b[39mSummaryMetadata\u001b[38;5;241m.\u001b[39mPluginData(\n\u001b[1;32m    325\u001b[0m         plugin_name\u001b[38;5;241m=\u001b[39mPLUGIN_NAME, content\u001b[38;5;241m=\u001b[39mcontent\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: value should be one of int, float, str, bool, or torch.Tensor"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Utils - Model management\n",
    "################################################################################################################\n",
    "def save_model_card(\n",
    "    repo_id: str,\n",
    "    images: Dict[str, Image.Image] = {},\n",
    "    base_model: str = None,\n",
    "    dataset_forget_name: str = None,\n",
    "    dataset_retain_name: str = None,\n",
    "    repo_folder: str = None,\n",
    "    eval_results: List[EvalResult] = [],  # whenever possible, should have this names: https://huggingface.co/metrics\n",
    "    tags: List[str] = [],\n",
    "    hyperparameters: dict = {},\n",
    "    similarities_gr: List[float] = [],\n",
    "    similarities_gf: List[float] = [],\n",
    "):\n",
    "    '''\n",
    "    the resulting file looks like this: https://github.com/huggingface/hub-docs/blob/main/modelcard.md\n",
    "    '''\n",
    "    os.makedirs(os.path.join(repo_folder, \"images\"), exist_ok=True)\n",
    "\n",
    "    img_str = \"\"\n",
    "    for name, image in images.items():\n",
    "        path_relative = os.path.join(\"images\", f\"{name}.png\")\n",
    "        image.save(os.path.join(repo_folder, path_relative))\n",
    "        img_str += f\"![img]({path_relative})\\n\"\n",
    "\n",
    "    # TODO: this description is not appearing in the model card\n",
    "    model_description = f\"\"\"\n",
    "# LoRA text2image fine-tuning - {repo_id}\n",
    "These are LoRA adaption weights for {base_model}. The weights were fine-tuned for forgetting {dataset_forget_name} dataset, while retaining {dataset_retain_name}. You can find some example images in the following. \\n\n",
    "{img_str}\n",
    "\"\"\"\n",
    "\n",
    "    model_card = load_or_create_model_card(\n",
    "        repo_id_or_path=repo_id,\n",
    "        from_training=True,\n",
    "        license=\"creativeml-openrail-m\",\n",
    "        base_model=base_model,\n",
    "        model_description=model_description,\n",
    "        inference=True,\n",
    "    )\n",
    "    model_card = populate_model_card(model_card, tags=tags)\n",
    "\n",
    "    model_card.data = ModelCardData(\n",
    "        model_name = repo_id,\n",
    "        eval_results=eval_results,\n",
    "        hyperparameters=hyperparameters,  # goes as kwargs\n",
    "    )\n",
    "\n",
    "    model_card.save(os.path.join(repo_folder, \"README.md\"))\n",
    "\n",
    "    with open(os.path.join(repo_folder, \"gradient_conflicts.json\"), \"w\") as f:\n",
    "        json.dump({\"forget\": similarities_gf, \"retain\": similarities_gr}, f)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Training uitls\n",
    "################################################################################################################\n",
    "def tokenize_captions(examples, tokenizer, caption_column, is_train=True):\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "\n",
    "def unwrap_model(model, accelerator):\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model = model._orig_mod if is_compiled_module(model) else model\n",
    "    return model\n",
    "\n",
    "def preprocess_train(examples, tokenizer, caption_column, image_column, train_transforms):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples, tokenizer, caption_column)\n",
    "    return examples\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "############################################\n",
    "# Unlearning algorithms\n",
    "############################################\n",
    "def unlearn_lora(model_original_id: str, model_lora_id: str, device: str) -> Tuple[StableDiffusionPipeline, StableDiffusionPipeline, StableDiffusionPipeline]:\n",
    "    '''\n",
    "    id can be both a local dir or a huggingface model id\n",
    "    return pipeline_original, pipeline_learned, pipeline_unlearned\n",
    "    '''\n",
    "    pipeline_original = AutoPipelineForText2Image.from_pretrained(model_original_id, torch_dtype=torch.float16, safety_checker=None).to(device)\n",
    "\n",
    "    pipeline_learned = AutoPipelineForText2Image.from_pretrained(model_original_id, torch_dtype=torch.float16, safety_checker=None).to(device)\n",
    "    pipeline_learned.load_lora_weights(model_lora_id, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "    pipeline_unlearned = AutoPipelineForText2Image.from_pretrained(model_original_id, torch_dtype=torch.float16, safety_checker=None).to(device)\n",
    "    pipeline_unlearned.load_lora_weights(model_lora_id, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "    total: int = 0\n",
    "    sum_before_invert: float = sum([float(param.sum()) for name, param in pipeline_unlearned.unet.named_parameters() if \"lora_A\" in name])\n",
    "    for name, param in pipeline_unlearned.unet.named_parameters():\n",
    "        if \"lora_A\" in name:\n",
    "            logger.debug(f\"Inverting param {name}\")\n",
    "            param.data = -1 * param.data\n",
    "            total += 1\n",
    "    assert sum_before_invert == -sum([float(param.sum()) for name, param in pipeline_unlearned.unet.named_parameters() if \"lora_A\" in name])\n",
    "    assert total > 0\n",
    "    logger.debug(f\"Inverted {total} params\")\n",
    "\n",
    "    return pipeline_original, pipeline_learned, pipeline_unlearned\n",
    "\n",
    "############################################\n",
    "# Evaluation utilities\n",
    "############################################\n",
    "def log_validation(\n",
    "    pipeline,\n",
    "    accelerator,\n",
    "    epoch,\n",
    "    num_validation_images,\n",
    "    validation_prompt,\n",
    "    seed,\n",
    "    is_final_validation=False,\n",
    ") -> Dict[str, Image.Image]:\n",
    "    images: Dict[str, Image.Image] = {}\n",
    "    logger.info(\n",
    "        f\"Running validation... \\n Generating {num_validation_images} images with prompt:\"\n",
    "        f\" {validation_prompt}.\"\n",
    "    )\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "    generator = torch.Generator(device=accelerator.device)\n",
    "    if seed is not None:\n",
    "        generator = generator.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        autocast_ctx = nullcontext()\n",
    "    else:\n",
    "        autocast_ctx = torch.autocast(accelerator.device.type)\n",
    "\n",
    "    with autocast_ctx:\n",
    "        for i in range(num_validation_images):\n",
    "            images[f\"val_prompt_{i+1:02d}\"] = pipeline(validation_prompt, num_inference_steps=30, generator=generator).images[0]\n",
    "\n",
    "    for tracker in accelerator.trackers:\n",
    "        phase_name = \"test\" if is_final_validation else \"validation\"\n",
    "        if tracker.name == \"tensorboard\":\n",
    "            tensorboard_log_image(tracker, phase_name, validation_prompt, epoch, images)\n",
    "        if tracker.name == \"wandb\":\n",
    "            wandb_log_image(tracker, phase_name, validation_prompt, epoch, images)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def plot_gradient_conflict_hist(similarities: List[float], title: str, color: str) -> Image.Image:\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    plt.hist(similarities, bins=50, color=color, alpha=0.75, label=\"Values\")\n",
    "    plt.axvline(np.mean(similarities), color=color, linestyle='-.', linewidth=2, label=\"Avgerage\")\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    #plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    fig.canvas.draw()\n",
    "    return Image.fromarray(np.uint8(np.array(fig.canvas.buffer_rgba())))\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# \n",
    "################################################################################################################\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "DATASET_NAME_MAPPING = {  # TODO: is this necessary?\n",
    "    \"lambdalabs/naruto-blip-captions\": (\"image\", \"text\"),\n",
    "    \"Hamdy20002/COCO_Person\": (\"image\", \"text\"),\n",
    "}\n",
    "\n",
    "class Trainer(BaseModel):\n",
    "    pass\n",
    "\n",
    "class TrainerLora(Trainer):\n",
    "    \"\"\"\n",
    "    Fine-tuning script for Stable Diffusion for text2image with support for LoRA.\n",
    "    Strongly based on the huggingface example (see credits in the end)\n",
    "    \n",
    "    Downgrades from the original script:\n",
    "    - Support for snr_gamma was dropped\n",
    "    - idem scale_lr\n",
    "\n",
    "\n",
    "    # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n",
    "    #\n",
    "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "    # you may not use this file except in compliance with the License.\n",
    "    # You may obtain a copy of the License at\n",
    "    #\n",
    "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "    #\n",
    "    # Unless required by applicable law or agreed to in writing, software\n",
    "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "    # See the License for the specific language governing permissions and\n",
    "    # limitations under the License.\n",
    "    \"\"\"\n",
    "    pretrained_model_name_or_path: str = Field(..., description=\"Path to pretrained model or model identifier from huggingface.co/models.\")\n",
    "    revision: Optional[str] = Field(None, description=\"Revision of pretrained model identifier from huggingface.co/models.\")\n",
    "    variant: Optional[str] = Field(None, description=\"Variant of the model files of the pretrained model identifier from huggingface.co/models, e.g., fp16.\")\n",
    "    \n",
    "    dataset_forget_name: Optional[str] = Field(None, description=\"The name or path of the dataset to be forgotten.\")\n",
    "    dataset_retain_name: Optional[str] = Field(None, description=\"The name or path of the dataset to be retained.\")\n",
    "    dataset_forget_config_name: Optional[str] = Field(None, description=\"The config of the dataset for forgetting, leave as None if there's only one config.\")\n",
    "    dataset_retain_config_name: Optional[str] = Field(None, description=\"The config of the dataset for retaining, leave as None if there's only one config.\")\n",
    "\n",
    "    image_column: str = Field(\"image\", description=\"The column of the dataset containing an image.\")\n",
    "    caption_column: str = Field(\"text\", description=\"The column of the dataset containing a caption or a list of captions.\")\n",
    "\n",
    "    validation_prompt: Optional[str] = Field(None, description=\"A prompt that is sampled during training for inference.\")\n",
    "    num_validation_images: int = Field(4, description=\"Number of images to generate during validation with `validation_prompt`.\")\n",
    "    validation_epochs: int = Field(1, description=\"Run fine-tuning validation every X epochs.\")\n",
    "\n",
    "    max_train_samples: Optional[int] = Field(None, description=\"Limit the number of training examples for debugging or quicker training.\")\n",
    "    output_dir: str = Field(\"sd-model-finetuned-lora\", description=\"Output directory for model predictions and checkpoints.\")\n",
    "    cache_dir: Optional[str] = Field(None, description=\"Directory where downloaded models and datasets will be stored.\")\n",
    "    \n",
    "    seed: Optional[int] = Field(None, description=\"A seed for reproducible training.\")\n",
    "    resolution: int = Field(512, description=\"Resolution for input images.\")\n",
    "    center_crop: bool = Field(False, description=\"Whether to center crop the input images.\")\n",
    "    random_flip: bool = Field(False, description=\"Whether to randomly flip images horizontally.\")\n",
    "\n",
    "    train_batch_size: int = Field(16, description=\"Batch size per device for training.\")\n",
    "    num_train_epochs: int = Field(100, description=\"Number of training epochs.\")\n",
    "    max_train_steps: Optional[int] = Field(None, description=\"Total number of training steps, overrides num_train_epochs if provided.\")\n",
    "\n",
    "    gradient_accumulation_steps: int = Field(1, description=\"Number of steps to accumulate before performing backward/update pass.\")\n",
    "    gradient_checkpointing: bool = Field(False, description=\"Enable gradient checkpointing to save memory at the expense of slower backward pass.\")\n",
    "\n",
    "    learning_rate: float = Field(1e-4, description=\"Initial learning rate after warmup period.\")\n",
    "    lr_scheduler: str = Field(\"constant\", description=\"Scheduler type for learning rate.\")\n",
    "    lr_warmup_steps: int = Field(500, description=\"Number of warmup steps in the learning rate scheduler.\")\n",
    "\n",
    "    use_8bit_adam: bool = Field(False, description=\"Use 8-bit Adam optimizer from bitsandbytes.\")\n",
    "    allow_tf32: bool = Field(False, description=\"Allow TF32 on Ampere GPUs for potential training speed-up.\")\n",
    "\n",
    "    dataloader_num_workers: int = Field(0, description=\"Number of subprocesses for data loading.\")\n",
    "    adam_beta1: float = Field(0.9, description=\"Beta1 parameter for Adam optimizer.\")\n",
    "    adam_beta2: float = Field(0.999, description=\"Beta2 parameter for Adam optimizer.\")\n",
    "    adam_weight_decay: float = Field(1e-2, description=\"Weight decay for Adam optimizer.\")\n",
    "    adam_epsilon: float = Field(1e-8, description=\"Epsilon value for Adam optimizer.\")\n",
    "    max_grad_norm: float = Field(1.0, description=\"Maximum gradient norm.\")\n",
    "\n",
    "    push_to_hub: bool = Field(False, description=\"Push the model to Hugging Face Hub.\")\n",
    "    hub_token: Optional[str] = Field(None, description=\"Token for authentication to push to Model Hub.\")\n",
    "    hub_model_id: Optional[str] = Field(None, description=\"Repository name to sync with `output_dir`.\")\n",
    "\n",
    "    logging_dir: str = Field(\"logs\", description=\"Directory for TensorBoard logs.\")\n",
    "    mixed_precision: Optional[str] = Field(None, description=\"Use mixed precision training: 'fp16' or 'bf16'.\")\n",
    "    report_to: str = Field(\"tensorboard\", description=\"Logging integration for reporting results (e.g., tensorboard, wandb).\")\n",
    "\n",
    "    local_rank: int = Field(-1, description=\"Local rank for distributed training.\")\n",
    "    checkpointing_steps: int = Field(500, description=\"Save training state checkpoint every X updates.\")\n",
    "    checkpoints_total_limit: Optional[int] = Field(None, description=\"Maximum number of checkpoints to store.\")\n",
    "    resume_from_checkpoint: Optional[str] = Field(None, description=\"Resume training from a previous checkpoint.\")\n",
    "\n",
    "    enable_xformers_memory_efficient_attention: bool = Field(False, description=\"Use xformers for memory-efficient attention.\")\n",
    "    noise_offset: float = Field(0.0, description=\"Scale of noise offset.\")\n",
    "    rank: int = Field(4, description=\"Dimension of the LoRA update matrices.\")\n",
    "\n",
    "    final_eval_prompts_forget: str|List[str] = Field([], description=\"Prompts for final evaluation on the forget dataset (ModelHub identifier or directly the prompts).\")\n",
    "    final_eval_prompts_retain: str|List[str] = Field([], description=\"Prompts for final evaluation on the retain dataset (ModelHub identifier or directly the prompts).\")\n",
    "\n",
    "    def run(self):\n",
    "        t0 = time.time()\n",
    "        if self.report_to == \"wandb\" and self.hub_token is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token.\"\n",
    "                \" Please use `huggingface-cli login` to authenticate with the Hub.\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # Aceleartor config\n",
    "        accelerator_project_config = accelerate.utils.ProjectConfiguration(project_dir=self.output_dir, logging_dir=Path(self.output_dir, self.logging_dir))\n",
    "\n",
    "        accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
    "            mixed_precision=self.mixed_precision,\n",
    "            log_with=self.report_to,\n",
    "            project_config=accelerator_project_config,\n",
    "        )\n",
    "\n",
    "        if accelerator.is_local_main_process:\n",
    "            datasets.utils.logging.set_verbosity_warning()\n",
    "            transformers.utils.logging.set_verbosity_warning()\n",
    "            diffusers.utils.logging.set_verbosity_info()\n",
    "        else:\n",
    "            datasets.utils.logging.set_verbosity_error()\n",
    "            transformers.utils.logging.set_verbosity_error()\n",
    "            diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "        if self.seed is not None:\n",
    "            accelerate.utils.set_seed(self.seed)\n",
    "\n",
    "        if torch.backends.mps.is_available():\n",
    "            accelerator.native_amp = False\n",
    "        \n",
    "        logger.info(accelerator.state)\n",
    "\n",
    "        # Handle the repository creation\n",
    "        if accelerator.is_main_process:\n",
    "            if self.output_dir is not None:\n",
    "                os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "            if self.push_to_hub:\n",
    "                repo_id = create_repo(\n",
    "                    repo_id=self.hub_model_id or Path(self.output_dir).name, exist_ok=True, token=self.hub_token\n",
    "                ).repo_id\n",
    "\n",
    "        # Load scheduler, tokenizer and models.\n",
    "        noise_scheduler = DDPMScheduler.from_pretrained(self.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            self.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=self.revision\n",
    "        )\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            self.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=self.revision\n",
    "        )\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            self.pretrained_model_name_or_path, subfolder=\"vae\", revision=self.revision, variant=self.variant\n",
    "        )\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            self.pretrained_model_name_or_path, subfolder=\"unet\", revision=self.revision, variant=self.variant\n",
    "        )\n",
    "        # freeze parameters of models to save more memory\n",
    "        unet.requires_grad_(False)\n",
    "        vae.requires_grad_(False)\n",
    "        text_encoder.requires_grad_(False)\n",
    "\n",
    "        # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
    "        # as these weights are only used for inference, keeping weights in full precision is not required.\n",
    "        weight_dtype = torch.float32\n",
    "        if accelerator.mixed_precision == \"fp16\":\n",
    "            weight_dtype = torch.float16\n",
    "        elif accelerator.mixed_precision == \"bf16\":\n",
    "            weight_dtype = torch.bfloat16\n",
    "\n",
    "        unet_lora_config = LoraConfig(\n",
    "            r=self.rank,\n",
    "            lora_alpha=self.rank,\n",
    "            init_lora_weights=\"gaussian\",\n",
    "            target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "        )\n",
    "\n",
    "        # Move unet, vae and text_encoder to device and cast to weight_dtype\n",
    "        unet.to(accelerator.device, dtype=weight_dtype)\n",
    "        vae.to(accelerator.device, dtype=weight_dtype)\n",
    "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "        # Add adapter and make sure the trainable params are in float32.\n",
    "        unet.add_adapter(unet_lora_config)\n",
    "        if self.mixed_precision == \"fp16\":\n",
    "            # only upcast trainable parameters (LoRA) into fp32\n",
    "            cast_training_params(unet, dtype=torch.float32)\n",
    "\n",
    "        if self.enable_xformers_memory_efficient_attention:\n",
    "            if is_xformers_available():\n",
    "                import xformers\n",
    "\n",
    "                xformers_version = version.parse(xformers.__version__)\n",
    "                if xformers_version == version.parse(\"0.0.16\"):\n",
    "                    logger.warning(\n",
    "                        \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n",
    "                    )\n",
    "                unet.enable_xformers_memory_efficient_attention()\n",
    "            else:\n",
    "                raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "        lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
    "\n",
    "        if self.gradient_checkpointing:\n",
    "            unet.enable_gradient_checkpointing()\n",
    "\n",
    "        # Enable TF32 for faster training on Ampere GPUs,\n",
    "        # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-s\n",
    "        if self.allow_tf32:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        if self.use_8bit_adam:\n",
    "            try:\n",
    "                import bitsandbytes as bnb\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
    "                )\n",
    "\n",
    "            optimizer_cls = bnb.optim.AdamW8bit\n",
    "        else:\n",
    "            optimizer_cls = torch.optim.AdamW\n",
    "\n",
    "        optimizer = optimizer_cls(\n",
    "            lora_layers,\n",
    "            lr=self.learning_rate,\n",
    "            betas=(self.adam_beta1, self.adam_beta2),\n",
    "            weight_decay=self.adam_weight_decay,\n",
    "            eps=self.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Get the datasets: you can either provide your own training and evaluation files (see below)\n",
    "        # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "        # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "        # download the dataset.\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        # TODO\n",
    "        dataset_forget = load_dataset(\n",
    "            self.dataset_forget_name,\n",
    "            self.dataset_forget_config_name,\n",
    "            cache_dir=self.cache_dir,\n",
    "            data_dir=None,\n",
    "        )\n",
    "        dataset_retain = load_dataset(\n",
    "            self.dataset_retain_name,\n",
    "            self.dataset_retain_config_name,\n",
    "            cache_dir=self.cache_dir,\n",
    "            data_dir=None,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Preprocessing the datasets.\n",
    "        # We need to tokenize inputs and targets.\n",
    "        column_names = dataset_forget[\"train\"].column_names\n",
    "\n",
    "        # 6. Get the column names for input/target.\n",
    "        dataset_columns = DATASET_NAME_MAPPING.get(self.dataset_forget_name, None)\n",
    "        if self.image_column is None:\n",
    "            image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "        else:\n",
    "            image_column = self.image_column\n",
    "            if image_column not in column_names:\n",
    "                raise ValueError(\n",
    "                    f\"--image_column' value '{self.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "                )\n",
    "        if self.caption_column is None:\n",
    "            caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "        else:\n",
    "            caption_column = self.caption_column\n",
    "            if caption_column not in column_names:\n",
    "                raise ValueError(\n",
    "                    f\"--caption_column' value '{self.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "                )\n",
    "            \n",
    "\n",
    "        # Preprocessing the datasets.\n",
    "        train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(self.resolution) if self.center_crop else transforms.RandomCrop(self.resolution),\n",
    "                transforms.RandomHorizontalFlip() if self.random_flip else transforms.Lambda(lambda x: x),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        with accelerator.main_process_first():\n",
    "            if self.max_train_samples is not None:\n",
    "                dataset_forget[\"train\"] = dataset_forget[\"train\"].shuffle(seed=self.seed).select(range(self.max_train_samples))\n",
    "            # Set the training transforms\n",
    "            train_dataset_forget = dataset_forget[\"train\"].with_transform(lambda examples: preprocess_train(examples, tokenizer, caption_column, image_column, train_transforms))\n",
    "            train_dataset_retain = dataset_retain[\"train\"].with_transform(lambda examples: preprocess_train(examples, tokenizer, caption_column, image_column, train_transforms))\n",
    "\n",
    "        # DataLoaders creation:\n",
    "        train_forget_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset_forget,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.dataloader_num_workers,\n",
    "        )\n",
    "        train_retain_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset_retain,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.dataloader_num_workers,\n",
    "        )\n",
    "\n",
    "        # Scheduler and math around the number of training steps.\n",
    "        # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation.\n",
    "        num_warmup_steps_for_scheduler = self.lr_warmup_steps * accelerator.num_processes\n",
    "        if self.max_train_steps is None:\n",
    "            len_train_dataloader_after_sharding = math.ceil(len(train_forget_dataloader) / accelerator.num_processes)\n",
    "            num_update_steps_per_epoch = math.ceil(len_train_dataloader_after_sharding / self.gradient_accumulation_steps)\n",
    "            num_training_steps_for_scheduler = (\n",
    "                self.num_train_epochs * num_update_steps_per_epoch * accelerator.num_processes\n",
    "            )\n",
    "        else:\n",
    "            num_training_steps_for_scheduler = self.max_train_steps * accelerator.num_processes\n",
    "\n",
    "        lr_scheduler = get_scheduler(\n",
    "            self.lr_scheduler,\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=num_warmup_steps_for_scheduler,\n",
    "            num_training_steps=num_training_steps_for_scheduler,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Prepare everything with our `accelerator`.\n",
    "        unet, optimizer, train_forget_dataloader, lr_scheduler = accelerator.prepare(\n",
    "            unet, optimizer, train_forget_dataloader, lr_scheduler\n",
    "        )\n",
    "\n",
    "        # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "        num_update_steps_per_epoch = math.ceil(len(train_forget_dataloader) / self.gradient_accumulation_steps)\n",
    "        if self.max_train_steps is None:\n",
    "            self.max_train_steps = self.num_train_epochs * num_update_steps_per_epoch\n",
    "            if num_training_steps_for_scheduler != self.max_train_steps * accelerator.num_processes:\n",
    "                logger.warning(\n",
    "                    f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_forget_dataloader)}) does not match \"\n",
    "                    f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \"\n",
    "                    f\"This inconsistency may result in the learning rate scheduler not functioning properly.\"\n",
    "                )\n",
    "        # Afterwards we recalculate our number of training epochs\n",
    "        self.num_train_epochs = math.ceil(self.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "        # We need to initialize the trackers we use, and also store our configuration.\n",
    "        # The trackers initializes automatically on the main process.\n",
    "        if accelerator.is_main_process:\n",
    "            accelerator.init_trackers(\"text2image-fine-tune\", config={k: v for k, v in self.model_dump().items() if isinstance(v, (str, float, int, type(None)))})\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        # Train!\n",
    "        t2 = time.time()\n",
    "        total_batch_size = self.train_batch_size * accelerator.num_processes * self.gradient_accumulation_steps\n",
    "\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(f\"  Num examples = {len(train_dataset_forget)} + {len(train_dataset_retain)}\")\n",
    "        logger.info(f\"  Num Epochs = {self.num_train_epochs}\")\n",
    "        logger.info(f\"  Instantaneous batch size per device = {self.train_batch_size}\")\n",
    "        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "        logger.info(f\"  Gradient Accumulation steps = {self.gradient_accumulation_steps}\")\n",
    "        logger.info(f\"  Total optimization steps = {self.max_train_steps}\")\n",
    "        global_step = 0\n",
    "        first_epoch = 0\n",
    "\n",
    "        # Potentially load in the weights and states from a previous save\n",
    "        if self.resume_from_checkpoint:\n",
    "            if self.resume_from_checkpoint != \"latest\":\n",
    "                path = os.path.basename(self.resume_from_checkpoint)\n",
    "            else:\n",
    "                # Get the most recent checkpoint\n",
    "                dirs = os.listdir(self.output_dir)\n",
    "                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "                path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "            if path is None:\n",
    "                accelerator.print(\n",
    "                    f\"Checkpoint '{self.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "                )\n",
    "                self.resume_from_checkpoint = None\n",
    "                initial_global_step = 0\n",
    "            else:\n",
    "                accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "                accelerator.load_state(os.path.join(self.output_dir, path))\n",
    "                global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "                initial_global_step = global_step\n",
    "                first_epoch = global_step // num_update_steps_per_epoch\n",
    "        else:\n",
    "            initial_global_step = 0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(0, self.max_train_steps),\n",
    "            initial=initial_global_step,\n",
    "            desc=\"Steps\",\n",
    "            # Only show the progress bar once on each machine.\n",
    "            disable=not accelerator.is_local_main_process,\n",
    "        )\n",
    "\n",
    "        similarities_gr: List[float] = []  # Cosine similarlities between \\tilde g and g_r, one element per step update\n",
    "        similarities_gf: List[float] = []  # Cosine similarlities between \\tilde g and g_f, one element per step update\n",
    "\n",
    "        for epoch in range(first_epoch, self.num_train_epochs):\n",
    "            unet.train()\n",
    "            train_loss_forget = 0.0  # TODO: plot graph of losses after training\n",
    "            train_loss_retain = 0.0\n",
    "            for step, batch_forget in enumerate(train_forget_dataloader):\n",
    "                batch_retain = next(iter(train_retain_dataloader))\n",
    "                min_length = min(len(batch_forget[\"pixel_values\"]), len(batch_retain[\"pixel_values\"]))\n",
    "                batch_forget[\"pixel_values\"] = batch_forget[\"pixel_values\"][:min_length]\n",
    "                batch_retain[\"pixel_values\"] = batch_retain[\"pixel_values\"][:min_length]\n",
    "                batch_forget[\"input_ids\"] = batch_forget[\"input_ids\"][:min_length]\n",
    "                batch_retain[\"input_ids\"] = batch_retain[\"input_ids\"][:min_length]\n",
    "                assert batch_forget[\"pixel_values\"].shape == batch_retain[\"pixel_values\"].shape\n",
    "                \n",
    "                batch_forget[\"pixel_values\"] = batch_forget[\"pixel_values\"].to(accelerator.device)\n",
    "                batch_retain[\"pixel_values\"] = batch_retain[\"pixel_values\"].to(accelerator.device)\n",
    "                \n",
    "                batch_forget[\"input_ids\"] = batch_forget[\"input_ids\"].to(accelerator.device)\n",
    "                batch_retain[\"input_ids\"] = batch_retain[\"input_ids\"].to(accelerator.device)\n",
    "\n",
    "                with accelerator.accumulate(unet):\n",
    "                    # Convert images to latent space\n",
    "                    latents_forget = vae.encode(batch_forget[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents_forget = latents_forget * vae.config.scaling_factor\n",
    "\n",
    "                    latents_retain = vae.encode(batch_retain[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents_retain = latents_retain * vae.config.scaling_factor\n",
    "\n",
    "                    # Sample noise that we'll add to the latents\n",
    "                    noise_forget = torch.randn_like(latents_forget)\n",
    "                    noise_retain = torch.randn_like(latents_retain)\n",
    "                    if self.noise_offset:\n",
    "                        # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n",
    "                        noise_forget += self.noise_offset * torch.randn(\n",
    "                            (latents_forget.shape[0], latents_forget.shape[1], 1, 1), device=latents_forget.device\n",
    "                        )\n",
    "                        noise_retain += self.noise_offset * torch.randn(\n",
    "                            (latents_retain.shape[0], latents_retain.shape[1], 1, 1), device=latents_retain.device\n",
    "                        )\n",
    "\n",
    "                    bsz = latents_forget.shape[0]\n",
    "                    # Sample a random timestep for each image\n",
    "                    timesteps_forget = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents_forget.device)\n",
    "                    timesteps_forget = timesteps_forget.long()\n",
    "                    timesteps_retain = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents_retain.device)\n",
    "                    timesteps_retain = timesteps_retain.long()\n",
    "\n",
    "                    # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                    # (this is the forward diffusion process)\n",
    "                    noisy_latents_forget = noise_scheduler.add_noise(latents_forget, noise_forget, timesteps_forget)\n",
    "                    noisy_latents_retain = noise_scheduler.add_noise(latents_retain, noise_forget, timesteps_forget)\n",
    "\n",
    "                    # Get the text embedding for conditioning\n",
    "                    encoder_hidden_states_forget = text_encoder(batch_forget[\"input_ids\"], return_dict=False)[0]\n",
    "                    encoder_hidden_states_retain = text_encoder(batch_retain[\"input_ids\"], return_dict=False)[0]\n",
    "\n",
    "                    # Get the target for loss depending on the prediction type\n",
    "                    if self.prediction_type is not None:\n",
    "                        # set prediction_type of scheduler if defined\n",
    "                        noise_scheduler.register_to_config(prediction_type=self.prediction_type)\n",
    "\n",
    "                    if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                        target_forget = noise_forget\n",
    "                        target_retain = noise_retain\n",
    "                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                        target_forget = noise_scheduler.get_velocity(latents_forget, noise_forget, timesteps_forget)\n",
    "                        target_retain = noise_scheduler.get_velocity(latents_retain, noise_retain, timesteps_retain)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                    # Predict the noise residual and compute loss\n",
    "                    model_pred_forget = unet(noisy_latents_forget, timesteps_forget, encoder_hidden_states_forget, return_dict=False)[0]\n",
    "                    model_pred_retain = unet(noisy_latents_retain, timesteps_retain, encoder_hidden_states_retain, return_dict=False)[0]\n",
    "\n",
    "                    loss_forget = F.mse_loss(model_pred_forget.float(), target_forget.float(), reduction=\"mean\")  # This is a Tensor of shape [], aka is a float\n",
    "                    loss_retain = F.mse_loss(model_pred_retain.float(), target_retain.float(), reduction=\"mean\")\n",
    "\n",
    "                    # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                    train_loss_forget += accelerator.gather(loss_forget.repeat(self.train_batch_size)).mean().item() / self.gradient_accumulation_steps\n",
    "                    train_loss_retain += accelerator.gather(loss_retain.repeat(self.train_batch_size)).mean().item() / self.gradient_accumulation_steps\n",
    "\n",
    "                    #########################################\n",
    "                    # Backpropagate\n",
    "                    #########################################\n",
    "                    \n",
    "                    # This is how it was before the munba trick:\n",
    "                    #accelerator.backward(loss)\n",
    "                    #if accelerator.sync_gradients:\n",
    "                    #    params_to_clip = lora_layers\n",
    "                    #    accelerator.clip_grad_norm_(params_to_clip, self.max_grad_norm)\n",
    "                    #optimizer.step()\n",
    "                    #lr_scheduler.step()\n",
    "                    #optimizer.zero_grad()\n",
    "                    \n",
    "                    # This is with the munba trick:\n",
    "                    \n",
    "                    # Compute gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    accelerator.backward(loss_forget)\n",
    "                    grads_forget = [p.grad.clone() for p in unet.parameters() if p.requires_grad]  # This list has 256 elements; each element is a torch.Tensor of shapes like [4, 320], then [320, 4], then [4, 640], then [640, 4], etc\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    accelerator.backward(loss_retain)\n",
    "                    grads_retain = [p.grad.clone() for p in unet.parameters() if p.requires_grad]\n",
    "                    \n",
    "                    #for e in grads_forget:\n",
    "                    #    print(e.shape)\n",
    "\n",
    "                    # Stack gradients to form matrix G\n",
    "                    G = torch.stack([\n",
    "                        torch.cat([g.view(-1) for g in grads_retain]),\n",
    "                        torch.cat([g.view(-1) for g in grads_forget])\n",
    "                    ])\n",
    "                    K = G @ G.T  # Compute K = G^T G; It is a 2x2 tensor\n",
    "                    # K /= torch.norm(K)  # As recomended here: https://github.com/AvivNavon/nash-mtl/blob/main/methods/weight_methods.py#L231\n",
    "                    \n",
    "                    # Solve for α using narsh equation\n",
    "                    k11, k12, k22 = K[0, 0], K[0, 1], K[1, 1]\n",
    "                    alpha_retain = torch.sqrt((2 * k11 * k22 + k12 * torch.sqrt(k11 * k22)) / (k11**2 * k22 - k11 * k12**2))    # This is a Tensor of shape [], aka is a float\n",
    "                    alpha_forget = (1 - k11 * alpha_retain**2) / (k12 * alpha_retain)    \n",
    "                    alpha = torch.tensor([alpha_retain, alpha_forget]).reshape(2, 1)  # Typical values seem to be things like [0.0016, -0.0029]\n",
    "                    # print(\"Alpha in this iteration:\", alpha)\n",
    "\n",
    "                    G = G.to(accelerator.device)\n",
    "                    alpha = alpha.to(accelerator.device)\n",
    "                    \n",
    "                    scaled_grad = G.T @ alpha\n",
    "                    # scaled_grad /= 2*torch.abs(alpha).min()\n",
    "                    # scaled_grad /= 2*alpha.min()\n",
    "                    # scaled_grad /= torch.norm(alpha)\n",
    "\n",
    "                    similarities_gr.append(F.cosine_similarity(scaled_grad[:, 0], torch.cat([g.view(-1) for g in grads_retain]), dim=0).item())\n",
    "                    similarities_gf.append(F.cosine_similarity(scaled_grad[:, 0], torch.cat([g.view(-1) for g in grads_forget]), dim=0).item())\n",
    "\n",
    "                    # Overwrite gradients for the optimizer\n",
    "                    for param, update in zip((p for p in unet.parameters() if p.requires_grad), \n",
    "                                            torch.split(scaled_grad, [p.numel() for p in unet.parameters() if p.requires_grad])):\n",
    "                        param.grad = update.view(param.shape)\n",
    "\n",
    "                    # Gradient clipping\n",
    "                    if accelerator.sync_gradients:\n",
    "                        params_to_clip = lora_layers\n",
    "                        accelerator.clip_grad_norm_(params_to_clip, self.max_grad_norm)\n",
    "\n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    #########################################\n",
    "                    # End of Backpropagate\n",
    "                    #########################################\n",
    "\n",
    "                # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "                if accelerator.sync_gradients:\n",
    "                    progress_bar.update(1)\n",
    "                    global_step += 1\n",
    "                    accelerator.log({\"train_loss_forget\": train_loss_forget}, step=global_step)\n",
    "                    accelerator.log({\"train_loss_retain\": train_loss_retain}, step=global_step)\n",
    "                    train_loss_forget = 0.0\n",
    "                    train_loss_retain = 0.0\n",
    "\n",
    "                    if global_step % self.checkpointing_steps == 0:\n",
    "                        if accelerator.is_main_process:\n",
    "                            # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                            if self.checkpoints_total_limit is not None:\n",
    "                                checkpoints = os.listdir(self.output_dir)\n",
    "                                checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                                checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                                # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                                if len(checkpoints) >= self.checkpoints_total_limit:\n",
    "                                    num_to_remove = len(checkpoints) - self.checkpoints_total_limit + 1\n",
    "                                    removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                                    logger.info(\n",
    "                                        f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                                    )\n",
    "                                    logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                                    for removing_checkpoint in removing_checkpoints:\n",
    "                                        removing_checkpoint = os.path.join(self.output_dir, removing_checkpoint)\n",
    "                                        shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                            save_path = os.path.join(self.output_dir, f\"checkpoint-{global_step}\")\n",
    "                            accelerator.save_state(save_path)\n",
    "\n",
    "                            unwrapped_unet = unwrap_model(unet)\n",
    "                            unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
    "                                get_peft_model_state_dict(unwrapped_unet)\n",
    "                            )\n",
    "\n",
    "                            StableDiffusionPipeline.save_lora_weights(\n",
    "                                save_directory=save_path,\n",
    "                                unet_lora_layers=unet_lora_state_dict,\n",
    "                                safe_serialization=True,\n",
    "                            )\n",
    "\n",
    "                            logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "                logs = {\"step_loss\": loss_forget.detach().item(), \"step_loss_forget\": loss_forget.detach().item(), \"step_loss_retain\": loss_retain.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "                progress_bar.set_postfix(**logs)\n",
    "\n",
    "                if global_step >= self.max_train_steps:\n",
    "                    break\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if self.validation_prompt is not None and epoch % self.validation_epochs == 0:\n",
    "                    # create pipeline\n",
    "                    pipeline = DiffusionPipeline.from_pretrained(\n",
    "                        self.pretrained_model_name_or_path,\n",
    "                        unet=unwrap_model(unet),\n",
    "                        revision=self.revision,\n",
    "                        variant=self.variant,\n",
    "                        torch_dtype=weight_dtype,\n",
    "                    )\n",
    "                    _ = log_validation(pipeline, accelerator, epoch, self.num_validation_images, self.validation_prompt, self.seed)\n",
    "\n",
    "                    del pipeline\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "\n",
    "        images: Dict[str, Image] = {}\n",
    "        similarities_gr = list(filter(lambda e: not np.isnan(e), similarities_gr))  # TODO: why are there nan values?\n",
    "        similarities_gf = list(filter(lambda e: not np.isnan(e), similarities_gf))\n",
    "        images['histogram_conflict_gr'] = plot_gradient_conflict_hist(similarities_gr, r\"Cosine Similarity between $\\tilde{g}$ and $g_r$\", \"#1f77b4\")  # Another nice color: #f4b400\n",
    "        images['histogram_conflict_gf'] = plot_gradient_conflict_hist(similarities_gf, r\"Cosine Similarity between $\\tilde{g}$ and $g_f$\", \"#1f77b4\")\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        if accelerator.is_main_process:\n",
    "            # Save the lora layers\n",
    "            unet = unet.to(torch.float32)\n",
    "\n",
    "            unwrapped_unet = unwrap_model(unet)\n",
    "            unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))\n",
    "            StableDiffusionPipeline.save_lora_weights(\n",
    "                save_directory=self.output_dir,\n",
    "                unet_lora_layers=unet_lora_state_dict,\n",
    "                safe_serialization=True,\n",
    "            )\n",
    "\n",
    "            t3 = time.time()\n",
    "\n",
    "            # Final inference\n",
    "            # Load previous pipeline\n",
    "            if self.validation_prompt is not None:\n",
    "                pipeline = DiffusionPipeline.from_pretrained(\n",
    "                    self.pretrained_model_name_or_path,\n",
    "                    revision=self.revision,\n",
    "                    variant=self.variant,\n",
    "                    torch_dtype=weight_dtype,\n",
    "                )\n",
    "                pipeline.load_lora_weights(self.output_dir)  # load attention processors\n",
    "                images.update(log_validation(pipeline, accelerator, epoch, self.num_validation_images, self.validation_prompt, self.seed, is_final_validation=True))  # run inference\n",
    "\n",
    "\n",
    "            #################################\n",
    "            pipeline_original, pipeline_learned, pipeline_unlearned = unlearn_lora(self.pretrained_model_name_or_path, self.output_dir, device=accelerator.device)\n",
    "\n",
    "            evaluator = EvaluatorTextToImage(\n",
    "                pipeline_original,\n",
    "                pipeline_learned,\n",
    "                pipeline_unlearned,\n",
    "                eval_prompts_forget,\n",
    "                eval_prompts_retain,\n",
    "                metric_clip=MetricImageTextSimilarity(metrics=['clip']),\n",
    "            )\n",
    "\n",
    "            eval_results, images2 = evaluator.evaluate()\n",
    "            images.update(images2)\n",
    "\n",
    "            t4 = time.time()\n",
    "\n",
    "\n",
    "            metric_common_attributes = {\n",
    "                \"task_type\": \"text-to-image\",\n",
    "                \"dataset_type\": f\"forget-and-retain-together\",\n",
    "                \"dataset_name\": f\"{self.dataset_forget_name} (forget) and {self.dataset_retain_name} (retain) sets\",\n",
    "            }\n",
    "\n",
    "            eval_results.append(EvalResult(\n",
    "                metric_type = 'runtime',\n",
    "                metric_name = f'Runtime init seconds (~↓)',\n",
    "                metric_value = t1-t0,\n",
    "                **metric_common_attributes,\n",
    "            ))\n",
    "            eval_results.append(EvalResult(\n",
    "                metric_type = 'runtime',\n",
    "                metric_name = f'Runtime data loading seconds (~↓)',\n",
    "                metric_value = t2-t1,\n",
    "                **metric_common_attributes,\n",
    "            ))\n",
    "            eval_results.append(EvalResult(\n",
    "                metric_type = 'runtime',\n",
    "                metric_name = f'Runtime training seconds (↓)',\n",
    "                metric_value = t3-t2,\n",
    "                **metric_common_attributes,\n",
    "            ))\n",
    "            eval_results.append(EvalResult(\n",
    "                metric_type = 'runtime',\n",
    "                metric_name = f'Runtime eval seconds (~↓)',\n",
    "                metric_value = t4-t3,\n",
    "                **metric_common_attributes,\n",
    "            ))\n",
    "\n",
    "            ################################\n",
    "            if self.push_to_hub:\n",
    "                save_model_card(\n",
    "                    repo_id,\n",
    "                    images=images,\n",
    "                    base_model=self.pretrained_model_name_or_path,\n",
    "                    dataset_forget_name=self.dataset_forget_name,\n",
    "                    dataset_retain_name=self.dataset_retain_name,\n",
    "                    repo_folder=self.output_dir,\n",
    "                    eval_results=eval_results,\n",
    "                    tags = [\n",
    "                        \"stable-diffusion\",\n",
    "                        \"stable-diffusion-diffusers\",\n",
    "                        \"text-to-image\",\n",
    "                        \"diffusers\",\n",
    "                        \"diffusers-training\",\n",
    "                        \"lora\",\n",
    "                    ],\n",
    "                    hyperparameters={k: v for k, v in self.model_dump().items() if isinstance(v, (str, float, int, type(None)))},\n",
    "                    similarities_gr=similarities_gr,\n",
    "                    similarities_gf=similarities_gf,\n",
    "                )\n",
    "\n",
    "                upload_folder(\n",
    "                    repo_id=repo_id,\n",
    "                    folder_path=self.output_dir,\n",
    "                    commit_message=\"End of training\",\n",
    "                    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                )\n",
    "\n",
    "        accelerator.end_training()\n",
    "\n",
    "        logger.info('wow!')\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# \n",
    "################################################################################################################\n",
    "def launch_training(**kwargs):\n",
    "    # Initialize the Accelerator\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\", dynamo_backend=\"no\")\n",
    "\n",
    "    # Wrap your training function with the accelerator\n",
    "    with accelerator.local_main_process_first():\n",
    "        if accelerator.is_local_main_process:\n",
    "            trainer = TrainerLora(**kwargs)\n",
    "            trainer.run()\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "# Example usage\n",
    "launch_training(\n",
    "    pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    dataset_forget_name=\"./assets/imagenette_splits/n02979186/train_forget\",\n",
    "    dataset_retain_name=\"./assets/imagenette_splits/n02979186/train_retain\",\n",
    "    dataloader_num_workers=2,\n",
    "    resolution=512,\n",
    "    mixed_precision=\"fp16\",\n",
    "    train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler=\"cosine\",\n",
    "    lr_warmup_steps=0,\n",
    "    output_dir=f\"./assets/lora/imagenette_splits/n02979186\",\n",
    "    num_train_epochs=1,\n",
    "    validation_epochs=1,\n",
    "    checkpointing_steps=500,\n",
    "    validation_prompt=\"Picture of a church\",\n",
    "    num_validation_images=1,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerLora(\n",
    "    pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    dataset_forget_name=\"./assets/imagenette_splits/n02979186/train_forget\",\n",
    "    dataset_retain_name=\"./assets/imagenette_splits/n02979186/train_retain\",\n",
    "    dataloader_num_workers=2,\n",
    "    resolution=512,\n",
    "    mixed_precision=\"fp16\",\n",
    "    train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler=\"cosine\",\n",
    "    lr_warmup_steps=0,\n",
    "    output_dir=f\"./assets/lora/imagenette_splits/n02979186\",\n",
    "    num_train_epochs=1,\n",
    "    validation_epochs=1,\n",
    "    checkpointing_steps=500,\n",
    "    validation_prompt=\"Picture of a church\",\n",
    "    num_validation_images=1,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "vars(trainer.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alice', 'age': 30}\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "p = Person(\"Alice\", 30)\n",
    "print(vars(p))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vars(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
