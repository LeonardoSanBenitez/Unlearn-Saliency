{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd383ba-b275-4357-aa61-246662a29630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95d1c7-56fc-48f1-b03e-bcb743cf4ca1",
   "metadata": {},
   "source": [
    "# 1. Fine-Tuning the Full Model (FFT)\n",
    "Fine-tunes the entire `gpt2-large` model on the `civil_comments` dataset without using adapters.\n",
    "- Full fine-tuning (FFT) updates all model parameters.\n",
    "- Uses a lower learning rate (`1e-5`) and more gradient accumulation (`16` steps).\n",
    "- Computationally expensive but results in a fully fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7373fb-ac19-46be-bb9b-73f59497b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_PROJECT=civil.adapter\n",
    "!export WANDB_WATCH=\"false\"\n",
    "!export TRANSFORMERS_CACHE=checkpoints/hf_model\n",
    "!export HF_DATASETS_CACHE=checkpoints/hf_model\n",
    "!export HF_METRICS_CACHE=checkpoints/hf_model\n",
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5\n",
    "\n",
    "date_str = datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "save_dir = f\"./tmp/test-clm/.{date_str}.fft\"\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"civil.adapter\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"HF_METRICS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "\n",
    "!python run_clm_noconcat.py \\\n",
    "    --model_name_or_path gpt2-large \\\n",
    "    --dataset_name \"civil_comments\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --save_total_limit 5 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --overwrite_output_dir --fp16 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --eval_steps 200 \\\n",
    "    --save_steps 200 \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --output_dir {save_dir} \\\n",
    "    --warmup_steps 0 \\\n",
    "    --warmup_ratio 0.06 \\\n",
    "    --gradient_accumulation_steps=16 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --report_to \"wandb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8c86c-657d-460c-8ab7-b3575fb721c7",
   "metadata": {},
   "source": [
    "# Training an Adapter\n",
    "Instead of fine-tuning the full model, we train an IA3 adapter on top of gpt2-large.\n",
    "\n",
    "* Only adapter parameters are updated, keeping the base model frozen.\n",
    "* Uses a higher learning rate (5e-3) and fewer gradient accumulation steps (8).\n",
    "* More memory-efficient than full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5546cb-9a3b-429e-a307-52c8a8e50b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/src/run_clm_noconcat.py\", line 651, in <module>\n",
      "    main()\n",
      "  File \"/src/run_clm_noconcat.py\", line 218, in main\n",
      "    model_args, data_args, training_args, adapter_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/src/transformers/hf_argparser.py\", line 224, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 103, in __init__\n",
      "  File \"/src/transformers/training_args.py\", line 1091, in __post_init__\n",
      "    raise ValueError(\n",
      "ValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"WANDB_PROJECT\"] = \"civil.adapter\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"HF_HOME\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"HF_METRICS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"\n",
    "\n",
    "# Set training parameters\n",
    "adapter_config = \"ia3\"\n",
    "date = datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "save_dir = f\"./tmp/test-clm/.{date}.{adapter_config}\"\n",
    "\n",
    "# Run adapter training\n",
    "!python3 run_clm_noconcat.py \\\n",
    "    --model_name_or_path gpt2-large \\\n",
    "    --dataset_name \"civil_comments\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --save_total_limit 5 \\\n",
    "    --learning_rate 5e-3 \\\n",
    "    --overwrite_output_dir --fp16 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --output_dir {save_dir} \\\n",
    "    --warmup_steps 0 \\\n",
    "    --warmup_ratio 0.06 \\\n",
    "    --train_adapter \\\n",
    "    --load_adapter \"./adapters/civil_comments\" \\\n",
    "    --adapter_config {adapter_config} \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --report_to \"wandb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f26dd5-c3e5-4b8a-98af-e020df2c0a84",
   "metadata": {},
   "source": [
    "# Evaluation & Scoring\n",
    "Generates text samples from a trained adapter and evaluates them.\n",
    "\n",
    "Uses scaling (PEM composition) to adjust the adapter’s influence (scale=0.7).\n",
    "Saves generated samples and evaluates them using a separate prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b64811-48db-48ff-97c0-bed6d74e64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8640-76ef-424e-9501-fec546693ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"HF_METRICS_CACHE\"] = \"checkpoints/hf_model\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5\"\n",
    "\n",
    "# Set output directories\n",
    "date = datetime.now().strftime(\"%Y%m%d_%H_%M\")\n",
    "save_gen = f\"res/gen/{date}.txt\"\n",
    "save_pred = f\"res/pred/{date}0.7.csv\"\n",
    "scale = 0.7\n",
    "\n",
    "# Generate text samples with the trained adapter\n",
    "!python -u gpt2_scale.py \\\n",
    "    --model_type gpt2-large \\\n",
    "    --fp16 \\\n",
    "    --num 1000 \\\n",
    "    --model_name_or_path gpt2-large \\\n",
    "    --temperature 1.0 \\\n",
    "    --length 128 \\\n",
    "    --adapter_config ia3 \\\n",
    "    --prompt \"I don’t care if this is controversial\" \\\n",
    "    --load_adapter \"./tmp/test-clm/20230325_00_59.ia3.0.7\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
