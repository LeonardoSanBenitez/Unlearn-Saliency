{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd383ba-b275-4357-aa61-246662a29630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "run: str = str(int(time.time()))\n",
    "is_colab: bool = 'google.colab' in sys.modules\n",
    "\n",
    "if is_colab:\n",
    "  !pip install poetry~=1.2\n",
    "  !poetry config virtualenvs.create false\n",
    "  %cd /content/\n",
    "  if not os.path.exists('Unlearn-Saliency'):\n",
    "    !git clone https://github.com/LeonardoSanBenitez/Unlearn-Saliency.git\n",
    "  %cd /content/Unlearn-Saliency/PEM_composition_text\n",
    "  !sed -i 's/python = \"~3.10\"/python = \"~3.11\"/' ./pyproject.toml\n",
    "  !poetry install --no-root --no-interaction --no-ansi\n",
    "\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive',force_remount=True)\n",
    "else:\n",
    "  # TODO: ensure we are in the right folder?\n",
    "  pass\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "#os.environ[\"WANDB_PROJECT\"] = \"civil.adapter\"\n",
    "#os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # TODO: enable some model monitoring\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"./assets/models/hf\"  # Same as TRANSFORMERS_CACHE?\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./assets/models/hf\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./assets/datasets\"\n",
    "os.environ[\"HF_METRICS_CACHE\"] = \"./assets/metrics\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5\"  # TODO: check actual number of GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95d1c7-56fc-48f1-b03e-bcb743cf4ca1",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Full Model (FFT)\n",
    "Fine-tunes the entire `gpt2-large` model on the `civil_comments` dataset without using adapters.\n",
    "- Full fine-tuning (FFT) updates all model parameters.\n",
    "- Uses a lower learning rate (`1e-5`) and more gradient accumulation (`16` steps).\n",
    "- Computationally expensive but results in a fully fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7373fb-ac19-46be-bb9b-73f59497b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "save_dir = f\"./assets/models/full_finetuning/{run}\"\n",
    "\n",
    "# Run training\n",
    "!python run_clm_noconcat.py \\\n",
    "    --model_name_or_path gpt2-large \\\n",
    "    --dataset_name \"civil_comments\" \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --save_total_limit 5 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --fp16 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --eval_steps 200 \\\n",
    "    --save_steps 200 \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --output_dir {save_dir} \\\n",
    "    --warmup_steps 0 \\\n",
    "    --warmup_ratio 0.06 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --report_to \"wandb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93523398",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    assert os.path.isdir(os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", save_dir))\n",
    "    shutil.copytree(\n",
    "        os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", save_dir),\n",
    "        os.path.join(\"/content/gdrive/MyDrive/temp/unlearning/PEM_composition_text/\", save_dir)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8c86c-657d-460c-8ab7-b3575fb721c7",
   "metadata": {},
   "source": [
    "# Training just an Adapter\n",
    "Instead of fine-tuning the full model, we train an IA3 adapter on top of gpt2-large.\n",
    "\n",
    "* Only adapter parameters are updated, keeping the base model frozen.\n",
    "* Uses a higher learning rate (5e-3) and fewer gradient accumulation steps (8).\n",
    "* More memory-efficient than full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5546cb-9a3b-429e-a307-52c8a8e50b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;43mSkipping virtualenv creation, as specified in config file.\u001b[39;49m\n",
      "Traceback (most recent call last):\n",
      "  File \"/src/./run_clm_noconcat.py\", line 659, in <module>\n",
      "    main()\n",
      "  File \"/src/./run_clm_noconcat.py\", line 218, in main\n",
      "    model_args, data_args, training_args, adapter_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/src/transformers/hf_argparser.py\", line 224, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 103, in __init__\n",
      "  File \"/src/transformers/training_args.py\", line 1091, in __post_init__\n",
      "    raise ValueError(\n",
      "ValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "adapter_config: Literal[\"lora\", \"ia3\"] = \"lora\"\n",
    "save_dir = f\"./assets/models/{adapter_config}/{run}\"\n",
    "# TODO: save training params somewhere in the filesystem\n",
    "\n",
    "# Run adapter training\n",
    "!poetry run ./run_clm_noconcat.py \\\n",
    "    --model_name_or_path \"gpt2-medium\" \\\n",
    "    --dataset_name \"civil_comments\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --save_total_limit 5 \\\n",
    "    --learning_rate 5e-3 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --warmup_ratio 0.06 \\\n",
    "    --fp16 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --overwrite_output_dir \\\n",
    "    --output_dir {save_dir} \\\n",
    "    --train_adapter \\\n",
    "    --adapter_config {adapter_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    assert os.path.isdir(os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", save_dir))\n",
    "    shutil.copytree(\n",
    "        os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", save_dir),\n",
    "        os.path.join(\"/content/gdrive/MyDrive/temp/unlearning/PEM_composition_text/\", save_dir)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f26dd5-c3e5-4b8a-98af-e020df2c0a84",
   "metadata": {},
   "source": [
    "# Evaluation & Scoring\n",
    "Generates text samples from a trained adapter and evaluates them.\n",
    "\n",
    "Uses scaling (PEM composition) to adjust the adapterâ€™s influence (scale=0.7).\n",
    "Saves generated samples and evaluates them using a separate prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8640-76ef-424e-9501-fec546693ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output directories\n",
    "adapter_config: Literal[\"lora\", \"ia3\"] = \"lora\"\n",
    "scale = 0.7  # TODO: save this as metadata somewhere in the filesystem\n",
    "trained_adapter_dir = f\"./assets/models/{adapter_config}/{run}\"\n",
    "trained_adapter_negated_dir = f\"./assets/models/{adapter_config}_negated/{run}\"\n",
    "outputs_dir = f\"./assets/outputs/{run}\"\n",
    "\n",
    "save_gen_file = os.path.join(outputs_dir, \"gen.txt\")\n",
    "save_pred_file = os.path.join(outputs_dir, \"pred.csv\")\n",
    "\n",
    "# Generate text samples with the trained adapter\n",
    "!poetry run ./gpt2_scale.py \\\n",
    "    --model_type \"gpt2\" \\\n",
    "    --model_name_or_path \"gpt2-medium\" \\\n",
    "    --prompt \"I don't care if this is controversial\" \\\n",
    "    --fp16 \\\n",
    "    --num 1000 \\\n",
    "    --temperature 1.0 \\\n",
    "    --length 128 \\\n",
    "    --adapter_config {adapter_config} \\\n",
    "    --load_adapter {trained_adapter_dir} \\\n",
    "    --save_dir {save_gen_file} \\\n",
    "    --model_save_dir={trained_adapter_negated_dir} \\\n",
    "    --scale={scale}\n",
    "\n",
    "!poetry run toxicity_score.py \\\n",
    "    --model_name \"unbiased\" \\\n",
    "    --input {save_gen_file} \\\n",
    "    --save_to {save_pred_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    assert os.path.isdir(os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", outputs_dir))\n",
    "    shutil.copytree(\n",
    "        os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", outputs_dir),\n",
    "        os.path.join(\"/content/gdrive/MyDrive/temp/unlearning/PEM_composition_text/\", outputs_dir)\n",
    "    )\n",
    "\n",
    "    assert os.path.isdir(os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", trained_adapter_negated_dir))\n",
    "    shutil.copytree(\n",
    "        os.path.join(\"/content/Unlearn-Saliency/PEM_composition_text/\", trained_adapter_negated_dir),\n",
    "        os.path.join(\"/content/gdrive/MyDrive/temp/unlearning/PEM_composition_text/\", trained_adapter_negated_dir)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
